{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by Guanyu Lin - guanyul@kth.se, and Tawsiful Islam - tawsiful@kth.se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import problem1 as mz\n",
    "import problem1_bonus as mz_b\n",
    "\n",
    "maze_mat = np.array([\n",
    "                [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                [0, 0, 1, 0, 0, 1, 1, 1],\n",
    "                [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                [0, 1, 1, 1, 1, 1, 1, 0],\n",
    "                [0, 0, 0, 0, 1, 2, 0, 0]])\n",
    "start_pos = (0,0)\n",
    "minotaur_pos = (6,5)\n",
    "mz.draw_maze(maze_mat, start_pos, minotaur_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (a) MDP formulation\n",
    "\n",
    "We propose the following tuple MDP formulation: \n",
    "\n",
    "#### State space $\\mathcal{S}$\n",
    "We would need to model the state space with each state as a tuple. Note that we exclude the obstacles' position for player since these are impossible states to be in for the player. Formally, the state space is\n",
    "\n",
    "$$\\mathcal{S} = \\big\\lbrace (i_p,j_p),(i_m,j_m):\\textrm{such that the cell\n",
    "} (i_p,j_p) \\textrm{ is not in the wall}\\big\\rbrace.$$\n",
    "With $(i_p,j_p)$ represents the position of the player and the $(i_m,j_m)$ represents the position of the minotaur.\n",
    "\n",
    "#### Action space $\\mathcal{A}$\n",
    "We allow the player to chose to either move `left`, `right`, `down`, `up` or not move at all (`stay`) and the minotaur moves randomlly to `left`, `right`, `down`, `up` without possible to control.\n",
    "Formally, the action space is\n",
    "\n",
    "$$\\mathcal{A} = \\lbrace \\textrm{up}, \\textrm{ down}, \\textrm{ left}, \\textrm{ right}, \\textrm{ stay} \\rbrace.$$ \n",
    "- There is no explicit action space for the Minotaur since its movements are not under our control.\n",
    "\n",
    "#### Transition probabilities $\\mathcal{P}$\n",
    "Note that there is no randomness involved upon taking an action by the player. Since the minotaur can not stay and will always move. the Transition probabilities are,  \n",
    "\n",
    "- If at state $s(0)$ taking action  $a$ does not lead the player to a wall or an obstacle but to another state (or position) $s'$, then $\\mathbb{P}(s(0)',s(1)') \\vert (s(0),s(1)), a) = 1$. \n",
    "- If at state $s(0)$ taking action  $a$ leads the player to a wall or an obstacle, the player remains in his state (or position) $s$, then $\\mathbb{P}(s(0),s(1)') \\vert (s(0),s(1)), a) = 1$.\n",
    "- Remember: $S=((i_p,j_p),(i_m,j_m))$ So $s(0)=(i_p,j_p)$ represents player's posiiton and $s(1)$ represents minotaur's position.\n",
    "\n",
    "\n",
    "\n",
    "#### Rewards $\\mathcal{R}$\n",
    "The objective of the player is to find the exit of the maze while avoiding the obstacles.    \n",
    "   - If at state $s$, taking action $a$, leads the player to a wall or $(i_p,j_p)=(i_m,j_m)$  then $r(s,a) = -\\infty$\n",
    "   - If at state $s$, taking action $a$, leads the player to some other position in the maze that is not the exit nor a wall, then $r(s, a) = -1$. \n",
    "   - If at state $s$, taking action $a$, leads the player to the exit then $r(s ,a) = 0$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (b) Alternating round\n",
    "\n",
    "#### State space $\\mathcal{S}$\n",
    "Same as in task (a).\n",
    "\n",
    "#### Action space $\\mathcal{A}$\n",
    "Same as in task (a).\n",
    "\n",
    "#### Transition probabilities $\\mathcal{P}$\n",
    "Assume that for $T=0,2,4,...,x$ is player's round, and $T=1,3,5,...,2x+1$ is minotaur's round.\n",
    "- player's round: If at state $s(0)$ taking action  $a$ does not lead the player to a wall or an obstacle but to another state (or position) $s'$, then $\\mathbb{P}(s(0)',s(1)) \\vert (s(0),s(1)), a) = 1$. \n",
    "- player's round: If at state $s(0)$ taking action  $a$ leads the player to a wall or an obstacle, the player remains in his state (or position) $s$, then $\\mathbb{P}(s(0),s(1)) \\vert (s(0),s(1)), a) = 1$.\n",
    "- minotaur's round: $\\mathbb{P}(s(0),s(1)') \\vert (s(0),s(1)), a) = 1$.\n",
    "\n",
    "#### Rewards $\\mathcal{R}$\n",
    "Same as in task (a).\n",
    "\n",
    "#### Analyse\n",
    "Since the player observes the Minotaur's position before making a move, the player can make informed decisions based on the Minotaur's current location. \n",
    "So I believe that the Minotaur will less likely to catch the player if they are not moving simultaneously.\n",
    "\n",
    "\n",
    "## Task (c) Dynamic Programming\n",
    "\n",
    "We showed the policy by doing a animation of one episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment maze\n",
    "env = mz.Maze(maze_mat,None,False,False)\n",
    "# Finite horizon\n",
    "horizon = 20\n",
    "# Solve the MDP problem with dynamic programming \n",
    "V, policy= mz.dynamic_programming(env,horizon)\n",
    "method = 'DynProg'\n",
    "start  = ((0, 0),( 6, 5))\n",
    "path = env.simulate(start, policy, method)\n",
    "mz.animate_solution(maze_mat, path)\n",
    "#print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (d)\n",
    "We simulated siturations that allowing the minotaur to stay and without allowing it to stay by 10000 times with each T from 1 - 30, and check if the player's final location is at the exit. If it is, we mark this simulation as a success. If by any frame, the minotaur and the player are in the same grid, we stop this episode directly without counting it as a success.\n",
    "\n",
    "When T is greater than 15 from both situration, we begin seeing success trials. Because it would take at least that many steps for the player to get to the exit.\n",
    "\n",
    "By comparing with and without the minotaur to take the stay action, we can observe that allowing the monster to stay has a significant lower success rate.\n",
    "\n",
    "The reason behind this can be that because the minotaur is swapning at the exit, and by allowing it to take the stay action, \n",
    "the probability that the minotaur stay around the exit will be much higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not allow monster to stay\n",
    "n = 10000\n",
    "ps = []\n",
    "for t in range(31):\n",
    "    V, policy= mz.dynamic_programming(env,t)\n",
    "    success = 0\n",
    "    method = 'DynProg'\n",
    "    start  = ((0, 0),(6, 5))\n",
    "    for _ in range(n):\n",
    "        path = env.simulate(start, policy, method);\n",
    "        if path[-1][0] == (6, 5):\n",
    "            success += 1\n",
    "    ps.append(success/n)\n",
    "\n",
    "plt.plot(ps, label=\"Without Allowing Monster to Stay\")\n",
    "\n",
    "# Allow monster to stay\n",
    "enva = mz.Maze(maze_mat)\n",
    "psa = []\n",
    "for t in range(31):\n",
    "    V, policy= mz.dynamic_programming(enva,t)\n",
    "    success = 0\n",
    "    method = 'DynProg'\n",
    "    start  = ((0, 0),(6, 5))\n",
    "    for _ in range(n):\n",
    "        path = enva.simulate(start, policy, method);\n",
    "        if path[-1][0] == (6, 5):\n",
    "            success += 1\n",
    "    psa.append(success/n)\n",
    "plt.plot(psa, label=\"Allowing Monster to Stay\")\n",
    "plt.legend()\n",
    "plt.title(\"Success Rate Over Different Time Horizons\")\n",
    "plt.xlabel(\"Time Horizon T\")\n",
    "plt.ylabel(\"Success Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (e) and (f) Value Iteration\n",
    "\n",
    "According to the task discribtion, the problem should now be formulated as an infinit horizon MDP. The Transition probabilities, Rewards are the same as previours.\n",
    "\n",
    "It is known that the player's life is geometrically distributed with mean 30. So we have: $\\mathbb{E}[T]=\\frac1{1-\\lambda}=30$ \n",
    "\n",
    "with objective function $\\max_{\\pi}\\lim_{T\\to\\infty}\\mathbb{E}[\\sum_{t=1}^Tr_t(S_t^\\pi,a_t^\\pi)]$\n",
    "\n",
    "\n",
    "When implementing the code, we add a condition in every iteration that $random.random() < 29/30$ to essentially checks if the generated random value falls below the threshold, simulating the geometric distribution of life with mean 30.\\\n",
    "\n",
    "The probability of getting out alive using this policy by simulating 10 000 games is 62%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_VI = mz.Maze(maze_mat,None,False,False)\n",
    "n = 10000\n",
    "success = 0\n",
    "v, policy = mz.value_iteration(env_VI, 29/30, 0.001)\n",
    "start  = ((0, 0),(6, 5))\n",
    "#path = env.simulate(start, policy, method)\n",
    "#mz.animate_solution(maze_mat, path)\n",
    "for _ in range(n):\n",
    "    path = env_VI.simulate(start, policy, 'ValIter')\n",
    "    if path[-1][0] == (6, 5):\n",
    "        success += 1\n",
    "print(\"The survice rate is: \"+str(100*success/n)+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (g)\n",
    "\n",
    "#### What does it mean that a learning method is on-policy or off-policy?\n",
    "\n",
    "- On-policy learning methods update the policy that is currently being used to make decisions. As the agent interacts with the environment, it updates its policy based on the outcomes of its actions. This means that the learning and decision-making processes are intertwined, and the policy that the agent uses to explore the environment is the same one that it is trying to improve.\n",
    "\n",
    "- Off-policy learning methods, on the other hand, learn about an optimal policy independently of the agent’s actions. They use data collected from a different policy to update their knowledge about the optimal policy. This means that the policy used for learning (the target policy) can be different from the policy used for decision-making (the behavior policy). An example of an off-policy learner is Q-learning.\n",
    "\n",
    "#### State the convergence conditions for Q-learning and SARSA.\n",
    "\n",
    "For Q-leaning\n",
    "- The learning rates must approach zero, but not too quickly. Formally, this requires that the sum of the learning rates must diverge, but the sum of their squares must converge.\n",
    "\n",
    "- Each state-action pair must be visited infinitely often. This has a precise mathematical definition: each action must have a non-zero probability of being selected by the policy in every state, i.e. π(s, a) > 0 for all (s, a). In practice, using an ε-greedy policy (where ε > 0) ensures that this condition is satisfied.\n",
    "\n",
    "For SARSA\n",
    "- The learning rate parameter α must satisfy the conditions: ∑αnk(s,a) = ∞ and ∑α2 nk(s,a) < ∞ ∀s ∈ S where nk(s, a) denotes the kth time (s, a) is visited.\n",
    "\n",
    "- The exploration parameter ε (of the ε-greedy policy) must be decayed so that the policy converges to a greedy policy.\n",
    "\n",
    "- Every state-action pair is visited infinitely many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task (h) MDP modification\n",
    "\n",
    "To modify the Markov Decision Process (MDP) for the new scenario, we would need to adjust the state space, transition probabilities, and reward function.\n",
    "\n",
    "#### State space $\\mathcal{S}$\n",
    "The state space would now need to include the information of the keys. So, a state could be defined as a tuple (player position, minotaur position and a key status indicator (To check weather the player has visited the key position)).\n",
    "\n",
    "#### Action space $\\mathcal{A}$\n",
    "The action space ramains the same, that player and minotaur can move into neibour blocks but no diagonally.\n",
    "\n",
    "#### Transition probabilities $\\mathcal{P}$\n",
    "The transition probabilities would need to be updated to reflect the new behavior of the minotaur. Now, with a 35% chance, the minotaur moves towards the player, and with a 65% chance, it moves uniformly at random in all directions.\n",
    "\n",
    "#### Rewards $\\mathcal{R}$\n",
    "The reward function would need to be adjusted to encourage the agent to first reach the keys at position C before heading to the exit at position B. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (i) Q-learning\n",
    "#### Describe your implementation in pseudo code\n",
    "\n",
    "```\n",
    "Initialize the Q-values and all n(s,a) for each (s,a) as zeros\n",
    "For each episode k from 1 to N\n",
    "    Initialize the environment and start from state s=s0 at time t=0\n",
    "    while episode k is not finished or not reached exit\n",
    "        with probability epsilon, choose random valid action a at s according to uniform distribution\n",
    "        else choose action a according to policy \n",
    "        n(s,a) <-- n(s,a) + 1\n",
    "        step <-- 1/n(s,a)**alpha where n(s,a) is number of times s has been visitied with action a\n",
    "        observe s and r\n",
    "        Q(s,a) <-- Q(s,a) + step * (r + gamma * max(Q(s',a')) - Q(s,a)) where a' gives highest Q-value at s'\n",
    "        t <-- t+1\n",
    "        s <-- s'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import problem1_bonus as mz_b\n",
    "maze_mat = np.array([\n",
    "                [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                [0, 0, 1, 0, 0, 1, 1, 1],\n",
    "                [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                [0, 1, 1, 1, 1, 1, 1, 0],\n",
    "                [0, 0, 0, 0, 1, 2, 0, 0]])\n",
    "start_pos = (0,0)\n",
    "minotaur_pos = (6,5)\n",
    "key_pos = (0,7)\n",
    "mz_b.draw_maze(maze_mat, start_pos, minotaur_pos,key_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning\n",
    "env = mz_b.Maze_bonus(maze_mat,None,False,False,True)\n",
    "n_states  = env.n_states\n",
    "n_actions = env.n_actions\n",
    "# Finite horizon\n",
    "no_episodes = 50000\n",
    "t_horizon = 200\n",
    "alpha = 2/3\n",
    "gamma = 49/50\n",
    "epsilon_list = [0.1,0.5]\n",
    "start  = ((0, 0),(6, 5),0)\n",
    "value_mat = [[],[]]\n",
    "Q_init = np.zeros((n_states, n_actions))\n",
    "# Solve the MDP problem with dynamic programming \n",
    "#Q, policy, value_list_1 = mz_b.Q_learning(env,Q_init,start,no_episodes,t_horizon,alpha,gamma,epsilon_list[0])\n",
    "#value_mat[0] = value_list_1\n",
    "Q, policy, value_list_2 = mz_b.Q_learning(env,Q_init,start,no_episodes,t_horizon,alpha,gamma,epsilon_list[1])\n",
    "value_mat[1] = value_list_2\n",
    "print(\"Simulation done\")\n",
    "method = 'ValIter'\n",
    "path = env.simulate(start, policy, method)\n",
    "# Show the shortest path \n",
    "print(path)\n",
    "mz_b.animate_solution(maze_mat, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA\n",
    "env = mz_b.Maze_bonus(maze_mat,None,False,False,False)\n",
    "n_states  = env.n_states\n",
    "n_actions = env.n_actions\n",
    "# Finite horizon\n",
    "no_episodes = 50000\n",
    "t_horizon = 300\n",
    "alpha = 2/3\n",
    "gamma = 49/50\n",
    "epsilon_list = [0.1,0.5]\n",
    "start  = ((0, 0),(6, 5),0)\n",
    "value_mat = [[],[]]\n",
    "Q_init = np.zeros((n_states, n_actions))\n",
    "# Solve the MDP problem with dynamic programming \n",
    "Q, policy, value_list_1 = mz_b.SARSA(env,Q_init,start,no_episodes,t_horizon,alpha,gamma,epsilon_list[0])\n",
    "value_mat[0] = value_list_1\n",
    "Q, policy, value_list_2 = mz_b.SARSA(env,Q_init,start,no_episodes,t_horizon,alpha,gamma,epsilon_list[1])\n",
    "value_mat[1] = value_list_2\n",
    "print(\"Simulation done\")\n",
    "method = 'ValIter'\n",
    "path = env.simulate(start, policy, method)\n",
    "# Show the shortest path \n",
    "print(path)\n",
    "mz_b.animate_solution(maze_mat, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(value_mat[0])\n",
    "plt.plot(value_mat[1])\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Episodes')\n",
    "plt.title('Q learning - gamma = '+str(gamma))\n",
    "plt.legend(['epsilon = 0.1 with zero initial', 'epsilon = 0.5 with zero initial',\n",
    "            'epsilon = 0.1 with non-zero initial', 'epsilon = 0.5 with non-zero initial'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_init = -np.random.uniform(1, 20, (n_states, n_actions))\n",
    "for s in range(n_states):\n",
    "    if env.states[s][0] == env.states[s][1] or (env.maze[env.states[s][0]]==2 and env.states[s][2] == 1):\n",
    "        Q_init[s,:]=np.zeros(n_actions)\n",
    "for epsilon in epsilon_list:\n",
    "    value_list = []\n",
    "    Q, policy, value_list = mz_b.Q_learning(env,Q_init,start,no_episodes,t_horizon,alpha,gamma,epsilon)\n",
    "    value_mat.append(value_list)\n",
    "    print(\"Simulation done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(value_mat[2])\n",
    "plt.plot(value_mat[3])\n",
    "plt.ylabel('V(initial state)')\n",
    "plt.xlabel('Episodes')\n",
    "plt.title('Q learning - epsilon = '+str(epsilon))\n",
    "plt.legend(['epsilon = 0.1 with zero initial', 'epsilon = 0.5 with zero initial',\n",
    "            'epsilon = 0.1 with non-zero initial', 'epsilon = 0.5 with non-zero initial'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_mat[1][-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
